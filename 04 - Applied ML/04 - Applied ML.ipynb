{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Applied ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import all the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146028, 28)\n"
     ]
    }
   ],
   "source": [
    "# import the data\n",
    "df = pd.read_csv('CrowdstormingDataJuly1st.csv')\n",
    "\n",
    "# initial shape of the data\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "playerShort      False\n",
       "player           False\n",
       "club             False\n",
       "leagueCountry    False\n",
       "birthday         False\n",
       "height            True\n",
       "weight            True\n",
       "position          True\n",
       "games            False\n",
       "victories        False\n",
       "ties             False\n",
       "defeats          False\n",
       "goals            False\n",
       "yellowCards      False\n",
       "yellowReds       False\n",
       "redCards         False\n",
       "photoID           True\n",
       "rater1            True\n",
       "rater2            True\n",
       "refNum           False\n",
       "refCountry       False\n",
       "Alpha_3           True\n",
       "meanIAT           True\n",
       "nIAT              True\n",
       "seIAT             True\n",
       "meanExp           True\n",
       "nExp              True\n",
       "seExp             True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check which columns have NaNs\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the column photoID since we won't use the photos of the players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.drop(['photoID'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove the column Alpha_3 since the same information seems to be represented by the column refCountry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['Alpha_3'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove records that have rater1 and/or rater2 as NaNs\n",
    "df = df.dropna(axis=0, subset=['rater1', 'rater2'])\n",
    "\n",
    "# create a new column with the mean of the two raters\n",
    "df['raterMean'] = (df['rater1'] + df['rater2']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124621, 27)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now focus on the referees. As stated in [this notebook](http://nbviewer.jupyter.org/github/mathewzilla/redcard/blob/master/Crowdstorming_visualisation.ipynb), we should only take into account referees that took part in games of one of the considered leagues. This means that referees with less than 22 dyads (corresponding to the, at least, 22 players of a game) shouldn't be relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "refs = df['refNum'].value_counts()\n",
    "\n",
    "df = df[df['refNum'].isin(refs[refs > 21].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: find a better way to clean the data\n",
    "# drop records with NaNs : drop NaNs is ok anyway randomforest tree needs to have no nans\n",
    "df.dropna(axis=0,inplace=True)\n",
    "#TODO: group results by player + performe the following op√©ration : remove id, name etc...\n",
    "df.drop(['playerShort','club','birthday','refNum','refCountry','player'],axis =1,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104420, 21)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# deals with labels : dummy codes .... need to clean a bit the df :D\n",
    "df_train = pd.get_dummies(df, prefix=['dummyCoding_','dummyCoding_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104420, 35)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain = df[['height', 'weight', 'games','victories','defeats','goals','meanIAT','nIAT','seIAT','meanExp','nExp','seExp']]\n",
    "Ytrain = np.asarray(df['raterMean'], dtype=\"|S6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name train_test_split",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-3c388261829f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# cross validation :\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;31m# split the datas :\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name train_test_split"
     ]
    }
   ],
   "source": [
    "# cross validation :\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# split the datas :\n",
    "randomState=random.randint(0,100)\n",
    "TrainingError =[]\n",
    "TestError = []\n",
    "\n",
    "for i in range(10) : \n",
    "    clf = RandomForestClassifier(n_estimators=10*i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xtrain, Ytrain, test_size=0.4, random_state=randomState)\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    train_error = 1.0 - clf.score(X_train, y_train)\n",
    "    test_error = clf.score(X_test,y_test)\n",
    "    TrainingError.append(train_error)\n",
    "    TestError.append(test_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on the training set: 0.123778969546\n"
     ]
    }
   ],
   "source": [
    "# error on the training set\n",
    "\n",
    "print('Error on the training set:', train_error)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
